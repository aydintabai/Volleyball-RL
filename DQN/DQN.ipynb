{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import gym\n",
        "import slimevolleygym\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "@dataclass\n",
        "class HyperParams:\n",
        "    BATCH_SIZE: int = 512\n",
        "    GAMMA: float = 0.99\n",
        "    EPS_START: float = 0.9\n",
        "    EPS_END: float = 0.05\n",
        "    EPS_DECAY: int = 8000  # Increased decay for more exploration\n",
        "    TAU: float = 0.01\n",
        "    LR: float = 5e-5\n",
        "    MEMORY_SIZE: int = 80000  # Increased memory size for more diverse experiences\n",
        "\n",
        "\n",
        "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\"))\n",
        "\n",
        "# set up interactive matplotlib\n",
        "is_ipython = \"inline\" in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "plt.ion()\n",
        "\n",
        "class SlimeVolleyWrapper:\n",
        "    \"\"\"\n",
        "    Wraps SlimeVolley so it can be used with the DQN implementation.\n",
        "    The original SlimeVolley has a continuous action space, but we'll discretize it.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env_name=\"SlimeVolley-v0\"):\n",
        "        \"\"\"\n",
        "        Initialize the SlimeVolley environment.\n",
        "\n",
        "        Args:\n",
        "            env_name (str): The name of the SlimeVolley environment to use.\n",
        "        \"\"\"\n",
        "        self.env = gym.make(env_name)\n",
        "\n",
        "        # Define a discrete action space for DQN\n",
        "        # SlimeVolley has 3 binary actions: LEFT, RIGHT, JUMP\n",
        "        # This results in 2^3 = 8 possible action combinations\n",
        "        self.action_space_n = 6\n",
        "\n",
        "        # Map from discrete action index to continuous action space\n",
        "        self.action_map = {\n",
        "            0: [0, 0, 0],  # NOOP\n",
        "            1: [1, 0, 0],  # LEFT\n",
        "            2: [0, 1, 0],  # RIGHT\n",
        "            3: [0, 0, 1],  # JUMP\n",
        "            4: [1, 0, 1],  # LEFT + JUMP\n",
        "            5: [0, 1, 1],  # RIGHT + JUMP\n",
        "        }\n",
        "\n",
        "        # Get observation space size\n",
        "        self.obs_size = self.env.observation_space.shape[0]\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the environment and return the initial observation.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: The initial observation.\n",
        "            dict: Empty dictionary for compatibility with Gym API.\n",
        "        \"\"\"\n",
        "        observation = self.env.reset()\n",
        "        return observation, {}\n",
        "\n",
        "    def step(self, action_index):\n",
        "        \"\"\"\n",
        "        Take a step in the environment with the given action.\n",
        "\n",
        "        Args:\n",
        "            action_index (int): The index of the action to take.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (observation, reward, done, truncated, info)\n",
        "        \"\"\"\n",
        "        action = self.action_map[action_index]\n",
        "        observation, reward, done, info = self.env.step(action)\n",
        "\n",
        "        return observation, reward, done, False, info\n",
        "\n",
        "\n",
        "class ReplayMemory:\n",
        "    \"\"\"\n",
        "    Replay memory to store transitions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, capacity: int):\n",
        "        \"\"\"Initialize the replay memory.\n",
        "\n",
        "        Args:\n",
        "            capacity (int): The maximum number of transitions to store.\n",
        "        \"\"\"\n",
        "        self.memory = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Save a transition\"\"\"\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        Sample a batch of transitions.\n",
        "\n",
        "        Args:\n",
        "            batch_size: The number of transitions to sample.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of sampled transitions.\n",
        "        \"\"\"\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, n_observations, n_actions):\n",
        "        \"\"\"\n",
        "        Initializes the DQN model.\n",
        "\n",
        "        Args:\n",
        "            n_observations (int): The size of the input observation space.\n",
        "            n_actions (int): The number of possible actions.\n",
        "        \"\"\"\n",
        "        super(DQN, self).__init__()\n",
        "        # Slimevolley has more complex observations, so we use a larger network\n",
        "        self.layer1 = nn.Linear(n_observations, 512)\n",
        "        self.ln1 = nn.LayerNorm(512)  # Replace BatchNorm with LayerNorm\n",
        "        self.layer2 = nn.Linear(512, 512)\n",
        "        self.ln2 = nn.LayerNorm(512)  # Replace BatchNorm with LayerNorm\n",
        "        self.layer3 = nn.Linear(512, 256)\n",
        "        self.ln3 = nn.LayerNorm(256)  # Replace BatchNorm with LayerNorm\n",
        "        self.layer4 = nn.Linear(256, n_actions)\n",
        "\n",
        "        # Initialize weights with Xavier initialization\n",
        "        nn.init.xavier_uniform_(self.layer1.weight)\n",
        "        nn.init.xavier_uniform_(self.layer2.weight)\n",
        "        nn.init.xavier_uniform_(self.layer3.weight)\n",
        "        nn.init.xavier_uniform_(self.layer4.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the DQN model.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor representing the state.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor representing Q-values for each action.\n",
        "        \"\"\"\n",
        "        # Handle both single and batch inputs\n",
        "        if x.dim() == 1:\n",
        "            x = x.unsqueeze(0)\n",
        "\n",
        "        x = F.relu(self.ln1(self.layer1(x)))\n",
        "        x = F.relu(self.ln2(self.layer2(x)))\n",
        "        x = F.relu(self.ln3(self.layer3(x)))\n",
        "        return self.layer4(x)\n",
        "\n",
        "\n",
        "class DQNTrainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        env: SlimeVolleyWrapper,\n",
        "        memory: ReplayMemory,\n",
        "        device: torch.device,\n",
        "        params: HyperParams,\n",
        "        max_steps_per_episode: int = 1000,\n",
        "        num_episodes: int = 2000,  # Increased for more training\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initializes the DQNTrainer with the required components to train a DQN agent.\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.policy_net = DQN(env.obs_size, env.action_space_n).to(device)\n",
        "        self.target_net = DQN(env.obs_size, env.action_space_n).to(device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.optimizer = optim.RMSprop(self.policy_net.parameters(), lr=params.LR)\n",
        "        self.memory = memory\n",
        "        self.device = device\n",
        "        self.params = params\n",
        "        self.max_steps_per_episode = max_steps_per_episode\n",
        "        self.num_episodes = num_episodes\n",
        "\n",
        "        # Track rewards per episode\n",
        "        self.episode_rewards = []\n",
        "        self.avg_rewards = []  # For tracking average rewards\n",
        "        self.steps_done = 0\n",
        "\n",
        "        # For evaluation\n",
        "        self.evaluation_rewards = []\n",
        "        self.eval_episodes = 100\n",
        "        self.eval_interval = 50  # Evaluate every 50 episodes\n",
        "\n",
        "    def select_action(self, state_tensor: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Selects an action using an epsilon-greedy policy based on current Q-network.\n",
        "        \"\"\"\n",
        "        # Compute epsilon threshold\n",
        "        sample = random.random()\n",
        "        eps_threshold = self.params.EPS_END + (self.params.EPS_START - self.params.EPS_END) * \\\n",
        "        math.exp(-1.0 * self.steps_done / self.params.EPS_DECAY)\n",
        "\n",
        "        # Update steps\n",
        "        self.steps_done += 1\n",
        "\n",
        "        # Exploit or explore\n",
        "        if sample > eps_threshold:\n",
        "            with torch.no_grad():\n",
        "                # Choose best action from Q-network\n",
        "                return self.policy_net(state_tensor).max(1).indices.view(1, 1)\n",
        "        else:\n",
        "            # Choose random action\n",
        "            return torch.tensor(\n",
        "                [[random.randrange(self.env.action_space_n)]],\n",
        "                device=self.device,\n",
        "                dtype=torch.long,\n",
        "            )\n",
        "\n",
        "    def optimize_model(self) -> None:\n",
        "        \"\"\"\n",
        "        Performs one gradient descent update on the policy network using Double DQN.\n",
        "        \"\"\"\n",
        "        if len(self.memory) < self.params.BATCH_SIZE:\n",
        "            return\n",
        "\n",
        "        transitions = self.memory.sample(self.params.BATCH_SIZE)\n",
        "        batch = Transition(*zip(*transitions))\n",
        "\n",
        "        state_batch = torch.cat(batch.state)\n",
        "        action_batch = torch.cat(batch.action)\n",
        "        reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "        non_final_mask = torch.tensor(\n",
        "            tuple(map(lambda s: s is not None, batch.next_state)),\n",
        "            device=self.device, dtype=torch.bool\n",
        "        )\n",
        "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
        "\n",
        "        # Get Q values for the current states and actions\n",
        "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "        # Initialize next state values to zero\n",
        "        next_state_values = torch.zeros(self.params.BATCH_SIZE, device=self.device)\n",
        "\n",
        "        # Double DQN implementation\n",
        "        with torch.no_grad():\n",
        "            # Get actions from policy network\n",
        "            next_action_indices = self.policy_net(non_final_next_states).max(1)[1].unsqueeze(1)\n",
        "            # Get Q values from target network for those actions\n",
        "            next_state_values[non_final_mask] = self.target_net(non_final_next_states).gather(1, next_action_indices).squeeze(1)\n",
        "\n",
        "        # Calculate expected Q values\n",
        "        expected_state_action_values = (next_state_values * self.params.GAMMA) + reward_batch\n",
        "\n",
        "        # Compute Huber loss\n",
        "        loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "        # Optimize the model\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # Clip gradients to avoid exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), max_norm=10)\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def soft_update(self) -> None:\n",
        "        \"\"\"\n",
        "        Performs a soft update of the target network parameters.\n",
        "        \"\"\"\n",
        "        target_net_state_dict = self.target_net.state_dict()\n",
        "        policy_net_state_dict = self.policy_net.state_dict()\n",
        "\n",
        "        for key in policy_net_state_dict:\n",
        "            target_net_state_dict[key] = policy_net_state_dict[key] * self.params.TAU + target_net_state_dict[key] * (1 - self.params.TAU)\n",
        "\n",
        "        self.target_net.load_state_dict(target_net_state_dict)\n",
        "\n",
        "    def evaluate(self) -> float:\n",
        "        \"\"\"\n",
        "        Evaluates the current policy by running several episodes.\n",
        "\n",
        "        Returns:\n",
        "            float: Average reward across evaluation episodes.\n",
        "        \"\"\"\n",
        "        total_reward = 0.0\n",
        "        for _ in range(self.eval_episodes):\n",
        "            obs, _ = self.env.reset()\n",
        "            state = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "            episode_reward = 0.0\n",
        "            done = False\n",
        "\n",
        "            for _ in range(self.max_steps_per_episode):\n",
        "                with torch.no_grad():\n",
        "                    action = self.policy_net(state).max(1).indices.view(1, 1)\n",
        "                next_obs, reward, done, _, _ = self.env.step(action.item())\n",
        "                episode_reward += reward\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "                state = torch.tensor(next_obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "\n",
        "            total_reward += episode_reward\n",
        "\n",
        "        avg_reward = total_reward / self.eval_episodes\n",
        "        self.evaluation_rewards.append(avg_reward)\n",
        "        return avg_reward\n",
        "\n",
        "    def plot_rewards(self, show_result: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        Plots accumulated rewards for each episode.\n",
        "        \"\"\"\n",
        "        plt.figure(1)\n",
        "        rewards_t = torch.tensor(self.episode_rewards, dtype=torch.float)\n",
        "\n",
        "        # Calculate moving average\n",
        "        if len(rewards_t) >= 100:\n",
        "            means = rewards_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "            means = torch.cat((torch.zeros(99), means))\n",
        "            self.avg_rewards = means.numpy()\n",
        "\n",
        "        # Decide whether to clear figure or show final result\n",
        "        if show_result:\n",
        "            plt.title(\"Result\")\n",
        "        else:\n",
        "            plt.clf()\n",
        "            plt.title(\"Training (Reward)\")\n",
        "\n",
        "        plt.xlabel(\"Episode\")\n",
        "        plt.ylabel(\"Reward\")\n",
        "        plt.plot(rewards_t.numpy(), alpha=0.6, label=\"Episode Reward\")\n",
        "\n",
        "\n",
        "        if len(self.evaluation_rewards) > 0:\n",
        "            eval_x = np.arange(0, len(self.episode_rewards), self.eval_interval)[:len(self.evaluation_rewards)]\n",
        "            plt.plot(eval_x, self.evaluation_rewards, 'r-', label=\"Evaluation Reward\")\n",
        "\n",
        "        plt.legend()\n",
        "        plt.pause(0.001)\n",
        "        if is_ipython:\n",
        "            if not show_result:\n",
        "                display.display(plt.gcf())\n",
        "                display.clear_output(wait=True)\n",
        "            else:\n",
        "                display.display(plt.gcf())\n",
        "\n",
        "    def train(self) -> None:\n",
        "            \"\"\"\n",
        "            Runs the main training loop across the specified number of episodes.\n",
        "            \"\"\"\n",
        "            best_eval_reward = float('-inf')\n",
        "            best_model_path = \"best_slimevolley_dqn_improved.pt\"\n",
        "\n",
        "            for i_episode in range(self.num_episodes):\n",
        "                # Reset the environment and initialize state and episode_reward\n",
        "                obs, _ = self.env.reset()\n",
        "                state = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "                episode_reward = 0.0\n",
        "\n",
        "                for t in range(self.max_steps_per_episode):\n",
        "                    # Select an action\n",
        "                    action = self.select_action(state)\n",
        "                    next_obs, reward, done, _, _ = self.env.step(action.item())\n",
        "\n",
        "                    # Convert observations to tensor\n",
        "                    next_state = torch.tensor(next_obs, dtype=torch.float32, device=self.device).unsqueeze(0) if not done else None\n",
        "\n",
        "                    # Save the transition in replay memory\n",
        "                    self.memory.push(state, action, next_state, torch.tensor([reward], device=self.device))\n",
        "\n",
        "                    # Advance state to next_state\n",
        "                    state = next_state if next_state is not None else torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "\n",
        "                    # Run optimization step\n",
        "                    self.optimize_model()\n",
        "\n",
        "                    # Perform soft update\n",
        "                    self.soft_update()\n",
        "\n",
        "                    # Accumulate the reward for the episode\n",
        "                    episode_reward += reward\n",
        "\n",
        "                    # Break the loop when a terminal state is reached\n",
        "                    if done:\n",
        "                        break\n",
        "\n",
        "                # Tracking episode reward and plotting rewards\n",
        "                self.episode_rewards.append(episode_reward)\n",
        "\n",
        "                # Print episode info\n",
        "                eps = self.params.EPS_END + (self.params.EPS_START - self.params.EPS_END) * math.exp(-1.0 * self.steps_done / self.params.EPS_DECAY)\n",
        "                print(f\"Episode {i_episode}: Reward = {episode_reward:.2f}, Epsilon = {eps:.4f}\")\n",
        "\n",
        "                # Evaluate periodically\n",
        "                if i_episode % self.eval_interval == 0:\n",
        "                    avg_eval_reward = self.evaluate()\n",
        "                    print(f\"Evaluation after episode {i_episode}: Average Reward = {avg_eval_reward:.2f}\")\n",
        "\n",
        "                    # Save the best model\n",
        "                    if avg_eval_reward > best_eval_reward:\n",
        "                        best_eval_reward = avg_eval_reward\n",
        "                        torch.save({\n",
        "                            'episode': i_episode,\n",
        "                            'model_state_dict': self.policy_net.state_dict(),\n",
        "                            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                            'reward': best_eval_reward,\n",
        "                            'epsilon': eps\n",
        "                        }, best_model_path)\n",
        "                        print(f\"New best model saved with reward {best_eval_reward:.2f}\")\n",
        "\n",
        "                # Update the rewards plot\n",
        "                if i_episode % 20 == 0:\n",
        "                    self.plot_rewards()\n",
        "\n",
        "            print(\"Training complete\")\n",
        "            print(f\"Best evaluation reward: {best_eval_reward:.2f}\")\n",
        "            self.plot_rewards(show_result=True)\n",
        "            plt.ioff()\n",
        "            plt.show()\n",
        "            plt.savefig(\"rewards_plot_slimevolley_dqn_improved.png\")\n",
        "\n",
        "            ## Define the path where you want to save the model in Google Drive\n",
        "            drive_model_path = \"/content/drive/My Drive/final_slimevolley_dqn_improved.pt\"\n",
        "\n",
        "            # Save final model to Google Drive\n",
        "            torch.save({\n",
        "                'episode': self.num_episodes,\n",
        "                'model_state_dict': self.policy_net.state_dict(),\n",
        "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                'reward': self.evaluation_rewards[-1] if len(self.evaluation_rewards) > 0 else -5.0,\n",
        "            }, drive_model_path)\n",
        "\n",
        "            print(f\"Final model saved to {drive_model_path}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Set up the environment and parameters\n",
        "    env = SlimeVolleyWrapper(env_name=\"SlimeVolley-v0\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Create hyperparameters\n",
        "    params = HyperParams(\n",
        "        BATCH_SIZE=512,\n",
        "        GAMMA=0.99,\n",
        "        EPS_START=0.9,\n",
        "        EPS_END=0.05,\n",
        "        EPS_DECAY=8000,  # Increased decay for more exploration\n",
        "        TAU=0.01,        # Increased TAU for faster target network updates\n",
        "        LR=5e-5,         # Reduced learning rate\n",
        "        MEMORY_SIZE=100000  # Increased memory size\n",
        "    )\n",
        "\n",
        "    # Create replay memory\n",
        "    memory = ReplayMemory(params.MEMORY_SIZE)\n",
        "\n",
        "    # Create trainer\n",
        "    trainer = DQNTrainer(\n",
        "        env=env,\n",
        "        memory=memory,\n",
        "        device=device,\n",
        "        params=params,\n",
        "        max_steps_per_episode=1000,\n",
        "        num_episodes=1000  # Increased training episodes\n",
        "    )\n",
        "\n",
        "    # Start training\n",
        "    trainer.train()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 829
        },
        "id": "-r3MjyV0ZHGr",
        "outputId": "801fa09a-ad5d-4afd-93f7-11f9aca464ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 241: Reward = -5.00, Epsilon = 0.0500\n",
            "Episode 242: Reward = -5.00, Epsilon = 0.0500\n",
            "Episode 243: Reward = -5.00, Epsilon = 0.0500\n",
            "Episode 244: Reward = -5.00, Epsilon = 0.0500\n",
            "Episode 245: Reward = -5.00, Epsilon = 0.0500\n",
            "Episode 246: Reward = -5.00, Epsilon = 0.0500\n",
            "Episode 247: Reward = -5.00, Epsilon = 0.0500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_from_drive(model_path, device):\n",
        "    \"\"\"\n",
        "    Loads a model from Google Drive.\n",
        "    \"\"\"\n",
        "    # Create the DQN\n",
        "    env = SlimeVolleyWrapper(env_name=\"SlimeVolley-v0\")\n",
        "    policy_net = DQN(env.obs_size, env.action_space_n).to(device)\n",
        "\n",
        "    # Load the saved model weights\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    policy_net.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Set to evaluation mode\n",
        "    policy_net.eval()\n",
        "\n",
        "    print(f\"Model loaded from {model_path}\")\n",
        "    print(f\"Checkpoint from episode {checkpoint['episode']} with reward {checkpoint.get('reward', 'unknown')}\")\n",
        "\n",
        "    return policy_net"
      ],
      "metadata": {
        "id": "901fRtILZRK5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # For training:\n",
        "    # [Your existing training code]\n",
        "\n",
        "    # For loading a saved model:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    drive_model_path = \"/content/drive/MyDrive/SlimeVolley/best_slimevolley_dqn_improved.pt\"\n",
        "\n",
        "    if os.path.exists(drive_model_path):\n",
        "        print(\"Loading model from Google Drive...\")\n",
        "        policy_net = load_model_from_drive(drive_model_path, device)\n",
        "        # Use the loaded model for evaluation or further training\n",
        "    else:\n",
        "        print(\"No saved model found in Google Drive, starting fresh training\")\n",
        "        # [Your training code]"
      ],
      "metadata": {
        "id": "-0hJ_Y5UOsXJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_against_builtin_ai():\n",
        "    # Setup\n",
        "    env = SlimeVolleyWrapper(env_name=\"SlimeVolley-v0\")  # Default opponent is built-in AI\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load your best model\n",
        "    best_model = load_trained_agent(\"best_slimevolley_dqn_improved.pt\", env, device)\n",
        "\n",
        "    # Evaluate against the built-in AI\n",
        "    print(\"Evaluating against built-in AI baseline...\")\n",
        "    evaluate_against_baseline(best_model, env, device, num_episodes=100)"
      ],
      "metadata": {
        "id": "NB6Z6eXJZmIw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_against_random_baseline():\n",
        "    env = SlimeVolleyWrapper(env_name=\"SlimeVolley-v0\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Create environment with random opponent\n",
        "    random_env = gym.make(\"SlimeVolley-v0\", opponent=\"random\")\n",
        "    random_env_wrapped = SlimeVolleyWrapper(env_name=None)\n",
        "    random_env_wrapped.env = random_env\n",
        "    random_env_wrapped.obs_size = random_env.observation_space.shape[0]\n",
        "\n",
        "    # Load your best model\n",
        "    best_model = load_trained_agent(\"best_slimevolley_dqn_improved.pt\", env, device)\n",
        "\n",
        "    # Evaluate against random policy\n",
        "    print(\"\\nEvaluating against random action baseline...\")\n",
        "    evaluate_against_baseline(best_model, random_env_wrapped, device, num_episodes=100)"
      ],
      "metadata": {
        "id": "WscStI51Z0aL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_learning_curves():\n",
        "    # Load your training data\n",
        "    # (You would need to save this data during training)\n",
        "\n",
        "    # Plot your learning curve\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(your_rewards, label=\"Your DQN Implementation\")\n",
        "\n",
        "    # Plot baseline learning curve if available\n",
        "    # plt.plot(baseline_rewards, label=\"Baseline DQN\")\n",
        "\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Average Reward\")\n",
        "    plt.title(\"Learning Curve Comparison\")\n",
        "    plt.legend()\n",
        "    plt.savefig(\"learning_curve_comparison.png\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "hiIsNHBYZ_Ca"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_gameplay(num_games=5):\n",
        "    env = gym.make(\"SlimeVolley-v0\", render_mode=\"human\")\n",
        "    wrapped_env = SlimeVolleyWrapper(env_name=None)\n",
        "    wrapped_env.env = env\n",
        "    wrapped_env.obs_size = env.observation_space.shape[0]\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    best_model = load_trained_agent(\"best_slimevolley_dqn_improved.pt\", wrapped_env, device)\n",
        "\n",
        "    for game in range(num_games):\n",
        "        obs, _ = wrapped_env.reset()\n",
        "        state = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Get action from your trained model\n",
        "            with torch.no_grad():\n",
        "                action = best_model(state).max(1).indices.view(1, 1)\n",
        "\n",
        "            # Execute action\n",
        "            next_obs, reward, done, _, _ = wrapped_env.step(action.item())\n",
        "\n",
        "            # Update state\n",
        "            if not done:\n",
        "                state = torch.tensor(next_obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "            # Add a small delay for better visualization\n",
        "            import time\n",
        "            time.sleep(0.05)"
      ],
      "metadata": {
        "id": "NX4sO051O4-1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main_evaluation():\n",
        "    # Test against built-in AI\n",
        "    test_against_builtin_ai()\n",
        "\n",
        "    # Test against random baseline\n",
        "    test_against_random_baseline()\n",
        "\n",
        "    # Compare learning curves\n",
        "    # compare_learning_curves()\n",
        "\n",
        "    # Visualize a few games\n",
        "    visualize_gameplay(num_games=5)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_evaluation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "b_hdSj0hO_Ac",
        "outputId": "00d699a2-1c60-4f42-c43b-8926488d1aeb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'gym' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-78b960eb5cbf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mmain_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-78b960eb5cbf>\u001b[0m in \u001b[0;36mmain_evaluation\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Test against built-in AI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtest_against_builtin_ai\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Test against random baseline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-b937d63ad634>\u001b[0m in \u001b[0;36mtest_against_builtin_ai\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_against_builtin_ai\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Setup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSlimeVolleyWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"SlimeVolley-v0\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Default opponent is built-in AI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-a60ec47cd450>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_name)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0menv_name\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mname\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mSlimeVolley\u001b[0m \u001b[0menvironment\u001b[0m \u001b[0mto\u001b[0m \u001b[0muse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \"\"\"\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Define a discrete action space for DQN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gym' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls *slimevolley*.pt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbVnl67gPBo6",
        "outputId": "765800b3-071e-4418-9909-d616a65e068b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '*slimevolley*.pt': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(\"Files in current directory:\")\n",
        "!ls *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsb6AfQPPFof",
        "outputId": "ff6d2b11-b099-4eb5-e05c-fa93845ecfe7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in current directory:\n",
            "anscombe.json\t\t     california_housing_train.csv  mnist_train_small.csv\n",
            "california_housing_test.csv  mnist_test.csv\t\t   README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xEhDf2zPKvh",
        "outputId": "3ea17bd6-c5d9-47cd-80af-5879f42666b2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: gym\n",
            "Version: 0.19.0\n",
            "Summary: The OpenAI Gym: A toolkit for developing and comparing your reinforcement learning agents.\n",
            "Home-page: https://github.com/openai/gym\n",
            "Author: OpenAI\n",
            "Author-email: gym@openai.com\n",
            "License: \n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: cloudpickle, numpy\n",
            "Required-by: dopamine_rl, slimevolleygym\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hOdn7u8PPg8J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}